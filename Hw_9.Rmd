---
title: "Hw_9"
author: "Pranita"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
#importing libraries and data sets
library(tidyverse)
library(mosaic)
library(moderndive)
library(effectsize)

soldering <- read_csv("solder.csv")
groceries <- read_csv("groceries.csv")

```

# name: Pranita Chaudhury
# UT ID: pc28377
# github: https://github.com/PranitaChau/Hw_9


# Problem 1
## Part A
```{r}
# opening vs skips graph 
ggplot(soldering) + 
  geom_jitter(aes(x = Opening, y = skips), width=0.1) + 
  stat_summary(aes(x = Opening, y = skips), fun='mean', color='red', size=1) + labs(title = "Opening vs Number of skips for soldering", x = "Opening Size", y= "Number of skips", caption = "Comparing small, medium, and large openings on solder gun. \n The average skips for small openings are 11, for medium openings are 4 and for large openings are 2.") + theme_classic()

mean(skips ~ Opening, data=soldering) |>
  round(0)


# solder vs skips graph 
ggplot(soldering) + 
  geom_jitter(aes(x = Solder, y = skips), width=0.1) + 
  stat_summary(aes(x = Solder, y = skips), fun='mean', color='red', size=1) + labs(title = "Soldering type vs Skips for soldering", x = "Solder thickness", caption = "Comparing thick and thin alloys used on solder guns. \n It can be noted that the average skips for thick alloys are 3, while it is 8 for thin alloys.") + theme_classic()


mean(skips ~ Solder, data=soldering) |>
  round(0)
```


## Part B


```{r}

#regression mmodel
relationship = lm(skips ~ Opening + Solder + Solder:Opening, data=soldering)
coef(relationship) |>
  round(0)



#bootstrap model for each coeff  
boot_skip <- do(100) * coef(lm(skips ~ Opening + Solder + Opening:Solder, data = resample(soldering)))

#make it a table
interval <- data.frame(apply(boot_skip, 2, quantile, probs = c(0.025, 0.975)))
interval
```



## write here 



## Part C
The baseline number of skips is 0, which represents boards with a large opening and thick solder. This intercept also refers to all interactions involving these baseline levels, since their coefficients are set to zero.
The main effect for medium Openings is 2 skips, this is the effect of OpeningM in isolation. The main effect for small Openings is 5 skips, this is the effect of OpeningS in isolation. The main effect for thin solders is 2 skips, this is the effect of SolderThin in isolation. 
The interaction effect for OpeningM and SolderThin is -1 skips. In other words, circuits which had both medium openings and thin solders used average 1 less skips than what you would expect from summing the individual “isolated” effects of the two variables. 
The interaction effect for OpeningS and SolderThin is 10 skips. In other words, circuits which had both small openings and thin solders used average 10 more skips than what you would expect from summing the individual “isolated” effects of the two variables. 




## Part D
If I had to recommend a combination of opening size and solder thickness to AT&T based on this analysis I would recommend they use a large opening and a thick solder since this is the most likely to result in 0 skips, which is the lowest number of skips possible.



# Problem 2
## Part A
```{r}

#bar graph in ascednding price order
price_store <- groceries |>
  group_by (Store) |>
  summarise(avg_price = mean(Price, na.rm = TRUE))

ggplot(price_store, aes(x = avg_price, y = reorder(Store, avg_price))) + geom_col(color = "navy", fill = "lightblue") + labs(title = "Average cost of items by store", x = "Price ($)", y = "Store", caption = "Graphing the average price of products sold at each store from highest to lowest.") + theme_classic()



```

## Part B

```{r}

#mutating dataset so 2 heb and 2 whole foods will the diff stores
groceries <- groceries |>
  mutate(Store_addy = paste(Store, Neighborhood, sep = "_"))

#wrangling data
is_product <- groceries |>
  group_by(Product) |>
  summarise(Num_stores = n_distinct(Store_addy)) |>
  arrange(desc(Num_stores)) 


#graphing w caption
ggplot(is_product, aes(x = Num_stores, y = reorder(Product, Num_stores))) + geom_col(color = "navy", fill = "lightblue") + labs(title = "Number of stores certain products are found at", x = "Product", y = "Number of Stores", caption = "Graphing the average price of products sold at each store from highest to lowest.") + theme_classic()

```



## Part C

```{r}

#regression + table
model = lm(Price ~ Product + Type, data= groceries)
get_regression_table(model, conf.level = 0.95, digits=2)

#showing only grocery 
tail(t)

```

Compared with ordinary grocery stores (like Albertsons, HEB, or Krogers), convenience stores charge somewhere between 0.41 and 0.92 dollars more for the same product.


## Part D
```{r}

#wrangling - making dummy variables 
groceries <- groceries |>
  mutate(Store = as.factor(Store),
         Product = as.factor(Product))

#regression + table
model = lm(Price ~ Store + Product, data= groceries)
store_prices <- get_regression_table(model, conf.level = 0.95, digits=2)

#first two stores have lowest price
store_prices |>
  filter(str_detect(term, "Store")) |>
  arrange(estimate) 


#first two stores will have hgihest price
store_prices |>
  filter(str_detect(term, "Store")) |>
  arrange(desc(estimate)) 

```


The two stores that seem to charge the lowest prices when comparing the same products are Whole Foods and Wheatsville Food Co-Op. The two stores that seem to charge the highest prices when comparing the same products are Walmart and Kroger Fresh Fare.



## Part E
```{r}

#making heb the baseline and refitting model
heb_groceries <- groceries |>
  mutate(Store = relevel(Store, ref = "H-E-B"))

model <- lm(Price ~ Store + Product, data = heb_groceries)
get_regression_table(model, conf.level = 0.95, digits=2)

#(printing out whole table so can compare other store /see relative expense)

```

Central Market on average charges $0.07 more for the same products as HEB. This difference is not a lot especially when compared to the other stores, so we can say that by relative difference Central Market is not price discriminating when looking at HEB prices.


## Part F
```{r}

#measuring income in multiples of 10,000
income_groceries <- groceries |>
  mutate("Income10K" = trunc(Income/10000))

#model
model = lm(Price ~ Income10K + Product, data= income_groceries)
imcome_pay <- get_regression_table(model, conf.level = 0.95, digits=2)

#looking at sign 
imcome_pay |>
  filter(str_detect(term, "Income10K"))

#standardizing variables
standardize_parameters(model)

```

Based on the sign of the Income10K variable, customers in poorer ZIP codes seem to pay more. I know thing because according to the model customers in poorer ZIP codes seem to pay 1 cent more for the same product on average for every $10,000 decrease in income. Since the confidence interval of -0.03 to 0.01 includes zero we cannot say this with certainty, but based on the sign of the variable alone we can say that as the income goes up the price goes down.

One-standard deviation increase in the income of a ZIP code seems to be associated with
a 0.03 standard-deviation change in the price that consumers in that ZIP code expect to pay for the same product.


# Problem 3

## Part A
This is true, as shown in the first graph (Fig A1) which compares the percent of minority residents in a ZIP code against the number of FAIR policies passed. This evidence shows that there is a positively related relationship with the number of FAIR policies and percentage of minority residents, so when the the minority percentage in a ZIP code goes up so does the number of FAIR policies passed. Additionally the simple linear regression performed below shows an r squared value of 0.5, so the data moderately fits the given equation indicating a positive moderately strong linear relationship.

## Part B
This statement is false. The linear regression model and graph (Figure B1 and the model underneath) only show an interaction between the age of the house and the minority percentage, there is no association in any of the given evidence that points to an interaction with these variables and number of FAIR policies. 

## Part C
This statement is true. In the Figure C1 the relationship between minority percentage and the number of FAIR policies for ever 100 households is shown. From this we can gather than there is a positive liner relationship between the two, and since the red line (high fire rick) is sightly steeper we can rather that it on average has a more FAIR policies enacted as the minority percentage goes up. The minority:fire_riskLow represents the effect of both being in a minority ZIP code and having a low fire risk, and since the regression model does not show the variable for being in a minority ZIP code and having a high fire risk (minority:fire_riskHigh), we can assume it is the intercept. Since the minority:fire_riskLow has a negative estimate it is lower than the intercept (minority:fire_riskHigh) meaning that high fire risk ZIP codes have more FAIR policies enacted per 100 households.

## Part D
This statement is false, adding income to the regression model changes the coefficients for the variables as well as the intercept, showing that income does have an effect on the association between FAIR policy uptake and the minority percentage.

## Part E
This statement is true. After controlling for other variables minority stil has a positive sign so it is postively related, and a small p value of 0.006 showing that it is statistically significant. This shows that after controlling for other variables in the dataset the number of FAIR policies still has a positve association with the minority percentage of a ZIP code.
